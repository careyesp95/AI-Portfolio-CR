import { Injectable } from '@nestjs/common';
import { OpenAIEmbeddings, ChatOpenAI } from '@langchain/openai';
import { Pinecone } from '@pinecone-database/pinecone';
import { StringOutputParser } from '@langchain/core/output_parsers';
import { ChatPromptTemplate, MessagesPlaceholder } from '@langchain/core/prompts';
import { Document } from '@langchain/core/documents';
import * as path from 'path';
import * as fs from 'fs';
import cliProgressBar from 'cli-progress';
import { PDFLoader } from '@langchain/community/document_loaders/fs/pdf';
import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';
import { PineconeStore } from "@langchain/community/vectorstores/pinecone";
import * as dotenv from 'dotenv';
import { AIMessage, BaseMessage, HumanMessage } from '@langchain/core/messages';
import { formatDocumentsAsString } from 'langchain/util/document';
import { RunnableSequence } from '@langchain/core/runnables';

dotenv.config();

@Injectable()
export class RagService {
  private llm: ChatOpenAI;
  private pinecone: Pinecone;
  private chatHistory: BaseMessage[] = [];

  constructor() {
    this.llm = new ChatOpenAI({
      apiKey: process.env.OPENAI_API_KEY!,
      model: 'gpt-3.5-turbo',
    });

    this.pinecone = new Pinecone({
      apiKey: process.env.PINECONE_API_KEY!,
    });
  }

  // 1Ô∏è‚É£ Load PDF documents
  private async loadDocuments(): Promise<Document[]> {
    const __dirname = path.resolve();
    const docsDir = path.join(__dirname, 'documents');
    const pdfFiles = fs.readdirSync(docsDir).filter(f => f.endsWith('.pdf'));

    if (pdfFiles.length === 0) {
      console.warn('‚ùå No PDF files found in documents folder.');
      return [];
    }

    console.log(`üöÄ Loading ${pdfFiles.length} PDF(s)...`);

    const progressBar = new cliProgressBar.SingleBar({
      format: 'Documents Loaded: {value}/{total}',
    });

    progressBar.start(pdfFiles.length, 0);

    const allDocs: Document[] = [];

    for (const file of pdfFiles) {
      const fullPath = path.join(docsDir, file);
      console.log(`üìÑ Loading ${fullPath}`);

      const loader = new PDFLoader(fullPath, { splitPages: true });
      const docs = await loader.load();
      allDocs.push(...docs);

      progressBar.increment();
    }

    progressBar.stop();
    console.log(`‚úÖ Loaded ${allDocs.length} total pages.`);
    return allDocs;
  }

  // 2Ô∏è‚É£ Chunking
  private async splitDocuments(rawDocs: Document[]) {
    const splitter = RecursiveCharacterTextSplitter.fromLanguage('html', {
      chunkSize: 500,
      chunkOverlap: 100,
    });

    const chunks = await splitter.splitDocuments(rawDocs);
    console.log(`üß© Created ${chunks.length} chunks.`);
    return chunks;
  }

  // 3Ô∏è‚É£ Vectorization
  private async vectorizeDocuments(docs: Document[]) {
    if (!docs.length) return '‚ö†Ô∏è No documents to vectorize.';

    console.log(`üöÄ Vectorizing ${docs.length} chunks...`);

    const embeddings = new OpenAIEmbeddings({
      model: 'text-embedding-3-small',
      apiKey: process.env.OPENAI_API_KEY!,
    });

    const index = this.pinecone.Index(process.env.PINECONE_INDEX!);
    const stats = await index.describeIndexStats();

    if ((stats.totalRecordCount || 0) > 0) {
      console.log('‚úÖ Index already populated. Skipping vectorization.');
      return '‚úÖ Index ready.';
    }

    const progress = new cliProgressBar.SingleBar({
      format: 'Vectorized: {value}/{total}',
    });
    progress.start(docs.length, 0);

    for (let i = 0; i < docs.length; i += 100) {
      const batch = docs.slice(i, i + 100);

      const sanitized = batch.map(doc => {
        if (doc.metadata?.date instanceof Date) {
          doc.metadata.date = doc.metadata.date.toISOString();
        }
        return doc;
      });

      await PineconeStore.fromDocuments(sanitized, embeddings, { pineconeIndex: index });
      progress.increment(batch.length);
    }

    progress.stop();
    console.log('‚úÖ Vectorization complete.');
    return '‚úÖ Done.';
  }

  // 4Ô∏è‚É£ Create Retriever (k=6 ‚úÖ)
  private async createRetriever() {
    const embeddings = new OpenAIEmbeddings({
      model: 'text-embedding-3-small',
      apiKey: process.env.OPENAI_API_KEY!,
    });

    const index = this.pinecone.Index(process.env.PINECONE_INDEX!);

    const store = await PineconeStore.fromExistingIndex(embeddings, {
      pineconeIndex: index,
    });

    console.log('‚úÖ Retriever ready.');

    return store.asRetriever({
      k: 6, // ‚úÖ bring more context
    });
  }

  // 5Ô∏è‚É£ Main Chat Flow
  async chatWithHistory(question: string): Promise<{ answer: string }> {
    const index = this.pinecone.Index(process.env.PINECONE_INDEX!);
    const stats = await index.describeIndexStats();

    if ((stats.totalRecordCount || 0) === 0) {
      console.warn('‚ùå Empty index. Running full RAG pipeline...');
      const raw = await this.loadDocuments();
      const chunks = await this.splitDocuments(raw);
      await this.vectorizeDocuments(chunks);
    }

    const retriever = await this.createRetriever();

    const llm = new ChatOpenAI({
      model: 'gpt-3.5-turbo',
      apiKey: process.env.OPENAI_API_KEY!,
    });

    // ‚úÖ your enhanced prompt stays intact
const prompt = ChatPromptTemplate.fromMessages([
  [
    "system",
    `You are Cristian Reyes' AI Portfolio Assistant.

    ‚úÖ GLOBAL BEHAVIOR
    - ALWAYS respond in the SAME language as the user.
    - NEVER translate or paraphrase the question.
    - NEVER say "I'm an AI" unless it's a general question NOT about Cristian.

    ‚úÖ PERSONAL QUESTIONS ‚Üí ALWAYS answer AS CRISTIAN
    These ALWAYS trigger Cristian persona mode:
      "how old are you", "what is your age", "when were you born", "where are you from",
      "tell me about yourself", "cu√©ntame sobre ti", "h√°blame de ti",
      "who are you", "about you", "sobre Cristian",
      "what technologies do you use", "what technologies does Cristian use",
      "qu√© tecnolog√≠as usas", "tech stack",
      "what skills do you have", "what are your skills",
      "your technologies"

    ‚ö†Ô∏è IMPORTANT:
    ‚Üí "your experience" was intentionally REMOVED to avoid overriding Experience section rules.

    ‚úÖ AGE RULE (VERY IMPORTANT)
    - When asked about Cristian‚Äôs age:
        ‚Üí Reply ONLY: "I was born on November 26, 1994. You can calculate my age from that date."
        ‚Üí Or in Spanish: "Nac√≠ el 26 de noviembre de 1994. Puedes calcular mi edad a partir de esa fecha."

    ‚úÖ EXPERIENCE DURATION RULE (STRICT)
    This rule activates ONLY when the user explicitly refers to YEARS or TIME LENGTH:
        ‚Üí Mentions ‚Äúyears‚Äù, ‚Äúa√±os‚Äù
        ‚Üí ‚Äúhow long‚Äù, ‚Äúcu√°nto tiempo‚Äù
        ‚Üí A number near ‚Äúexperience‚Äù
    Examples:
        - "How many years of experience do you have?"
        - "Cu√°ntos a√±os de experiencia tienes?"
        - "How long have you been working?"
        - "Do you have 3 years of experience?"

    If triggered:
        ‚Üí Reply ONLY: "I have over 3 years of experience in development."
          (or Spanish version)
        ‚Üí DO NOT use CV structured Experience section.

    ‚ö†Ô∏è MUST NOT trigger this rule:
        - "Tell me about your work experience"
        - "Do you have any experience of work?"
        - "Experience"
        - "Work experience"
        - "Has trabajado?"
        - "Tienes experiencia?"
        ‚Üí These must use the structured Experience section.

    ‚úÖ PORTFOLIO TECH STACK RULE
    When asked about the technologies Cristian used for his portfolio:
        ‚Üí Respond with the Skills section (structured)
        ‚Üí Then add the portfolio explanation paragraph
        ‚Üí Translate if necessary.

    ‚úÖ CV SECTIONS ‚Üí TRIGGER STRUCTURED FORMAT

    ‚û§ About Me  
    Triggers: ‚Äútell me about yourself‚Äù, ‚Äúcu√©ntame sobre ti‚Äù, ‚Äúabout you‚Äù, ‚Äúh√°blame de ti‚Äù
    Format:
      About Me
      [single paragraph]

    ‚û§ Skills  
    Triggers: ‚Äúskills‚Äù, ‚Äútechnologies‚Äù, ‚Äútech stack‚Äù, etc.
    Format:
      Skills
      Title: [Category]
      Description: [List]

    ‚û§ Experience  
    Triggers:
      ‚Äúexperience‚Äù, ‚Äúwork experience‚Äù, ‚Äúhas trabajado‚Äù, 
      ‚Äútienes experiencia‚Äù, ‚Äútell me about your work experience‚Äù
    Format:
      Experience
      Title: [Role]
      Company: [Company Name]
      Dates: [Start ‚Äì End]
      Description: [From CV only]

    ‚û§ Projects  
    Triggers: ‚Äúprojects‚Äù, ‚Äúportfolio‚Äù
    Format:
      Projects & Scientific Projects
      Title:
      Description:
      View Project:

    ‚û§ Contact  
    Triggers: ‚Äúcontact‚Äù, ‚Äúemail‚Äù, ‚ÄúLinkedIn‚Äù, ‚ÄúGitHub‚Äù, ‚Äúhow to reach you‚Äù
    Format:
      Contact
      Email:
      Location:
      GitHub:
      LinkedIn:

    ‚û§ Education  
    Format:
      Education
      Title:
      Description:
      Dates:

    ‚úÖ GENERAL QUESTIONS
    - Respond like a normal AI
    - DO NOT use Cristian‚Äôs CV

    -------------------------
    {context}
    -------------------------
    `
  ],
  new MessagesPlaceholder("chat_history"),
  ["human", "{question}"],
]);




    // ‚úÖ retrieval + cleanup of empty chunks
    const retrievalChain = RunnableSequence.from([
      input => input.question,
      retriever,
      async (docs: Document[]) =>
        docs
          .map(d => d.pageContent.trim())
          .filter(t => t.length > 30)
          .join("\n\n"),
    ]);

    const generationChain = RunnableSequence.from([
      {
        question: input => input.question,
        context: retrievalChain,
        chat_history: input => input.chat_history,
      },
      prompt,
      llm,
      new StringOutputParser(),
    ]);

    const answer = await generationChain.invoke({
      question,
      chat_history: this.chatHistory,
    });

    this.chatHistory.push(new HumanMessage(question));
    this.chatHistory.push(new AIMessage(answer));

    return { answer };
  }

  // 6Ô∏è‚É£ Clear chat
  async resetChatHistory() {
    this.chatHistory = [];
    console.log("‚úÖ Chat history cleared.");
    return "Chat history cleared.";
  }
}

