import { Injectable } from '@nestjs/common';
import { OpenAIEmbeddings, ChatOpenAI } from '@langchain/openai';
import { Pinecone } from '@pinecone-database/pinecone';
import { StringOutputParser } from '@langchain/core/output_parsers';
import { ChatPromptTemplate, MessagesPlaceholder } from '@langchain/core/prompts';
import { Document } from '@langchain/core/documents';
import * as path from 'path';
import * as fs from 'fs';
import cliProgressBar from 'cli-progress';
import { PDFLoader } from '@langchain/community/document_loaders/fs/pdf';
import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';
import { PineconeStore } from "@langchain/community/vectorstores/pinecone";
import * as dotenv from 'dotenv';
import { AIMessage, BaseMessage, HumanMessage } from '@langchain/core/messages';
import { formatDocumentsAsString } from 'langchain/util/document';
import { RunnableSequence } from '@langchain/core/runnables';

dotenv.config();

@Injectable()
export class RagService {
  private llm: ChatOpenAI;
  private pinecone: Pinecone;
  private chatHistory: BaseMessage[] = [];

  constructor() {
    this.llm = new ChatOpenAI({
      apiKey: process.env.OPENAI_API_KEY!,
      model: 'gpt-3.5-turbo',
    });

    this.pinecone = new Pinecone({
      apiKey: process.env.PINECONE_API_KEY!,
    });
  }

  // 1Ô∏è‚É£ Load PDF documents
  private async loadDocuments(): Promise<Document[]> {
    const __dirname = path.resolve();
    const docsDir = path.join(__dirname, 'documents');
    const pdfFiles = fs.readdirSync(docsDir).filter(f => f.endsWith('.pdf'));

    if (pdfFiles.length === 0) {
      console.warn('‚ùå No PDF files found in documents folder.');
      return [];
    }

    console.log(`üöÄ Loading ${pdfFiles.length} PDF(s)...`);

    const progressBar = new cliProgressBar.SingleBar({
      format: 'Documents Loaded: {value}/{total}',
    });

    progressBar.start(pdfFiles.length, 0);

    const allDocs: Document[] = [];

    for (const file of pdfFiles) {
      const fullPath = path.join(docsDir, file);
      console.log(`üìÑ Loading ${fullPath}`);

      const loader = new PDFLoader(fullPath, { splitPages: true });
      const docs = await loader.load();
      allDocs.push(...docs);

      progressBar.increment();
    }

    progressBar.stop();
    console.log(`‚úÖ Loaded ${allDocs.length} total pages.`);
    return allDocs;
  }

  // 2Ô∏è‚É£ Chunking
  private async splitDocuments(rawDocs: Document[]) {
    const splitter = RecursiveCharacterTextSplitter.fromLanguage('html', {
      chunkSize: 500,
      chunkOverlap: 100,
    });

    const chunks = await splitter.splitDocuments(rawDocs);
    console.log(`üß© Created ${chunks.length} chunks.`);
    return chunks;
  }

  // 3Ô∏è‚É£ Vectorization
  private async vectorizeDocuments(docs: Document[]) {
    if (!docs.length) return '‚ö†Ô∏è No documents to vectorize.';

    console.log(`üöÄ Vectorizing ${docs.length} chunks...`);

    const embeddings = new OpenAIEmbeddings({
      model: 'text-embedding-3-small',
      apiKey: process.env.OPENAI_API_KEY!,
    });

    const index = this.pinecone.Index(process.env.PINECONE_INDEX!);
    const stats = await index.describeIndexStats();

    if ((stats.totalRecordCount || 0) > 0) {
      console.log('‚úÖ Index already populated. Skipping vectorization.');
      return '‚úÖ Index ready.';
    }

    const progress = new cliProgressBar.SingleBar({
      format: 'Vectorized: {value}/{total}',
    });
    progress.start(docs.length, 0);

    for (let i = 0; i < docs.length; i += 100) {
      const batch = docs.slice(i, i + 100);

      const sanitized = batch.map(doc => {
        if (doc.metadata?.date instanceof Date) {
          doc.metadata.date = doc.metadata.date.toISOString();
        }
        return doc;
      });

      await PineconeStore.fromDocuments(sanitized, embeddings, { pineconeIndex: index });
      progress.increment(batch.length);
    }

    progress.stop();
    console.log('‚úÖ Vectorization complete.');
    return '‚úÖ Done.';
  }

  // 4Ô∏è‚É£ Create Retriever (k=6 ‚úÖ)
  private async createRetriever() {
    const embeddings = new OpenAIEmbeddings({
      model: 'text-embedding-3-small',
      apiKey: process.env.OPENAI_API_KEY!,
    });

    const index = this.pinecone.Index(process.env.PINECONE_INDEX!);

    const store = await PineconeStore.fromExistingIndex(embeddings, {
      pineconeIndex: index,
    });

    console.log('‚úÖ Retriever ready.');

    return store.asRetriever({
      k: 6, // ‚úÖ bring more context
    });
  }

  // 5Ô∏è‚É£ Main Chat Flow
  async chatWithHistory(question: string): Promise<{ answer: string }> {
    const index = this.pinecone.Index(process.env.PINECONE_INDEX!);
    const stats = await index.describeIndexStats();

    if ((stats.totalRecordCount || 0) === 0) {
      console.warn('‚ùå Empty index. Running full RAG pipeline...');
      const raw = await this.loadDocuments();
      const chunks = await this.splitDocuments(raw);
      await this.vectorizeDocuments(chunks);
    }

    const retriever = await this.createRetriever();

    const llm = new ChatOpenAI({
      model: 'gpt-3.5-turbo',
      apiKey: process.env.OPENAI_API_KEY!,
    });

    // ‚úÖ your enhanced prompt stays intact
const prompt = ChatPromptTemplate.fromMessages([
  [
    "system",
    `You are Cristian Reyes' AI Portfolio Assistant.

    ‚úÖ GLOBAL BEHAVIOR
    - ALWAYS respond in the SAME language as the user.
    - NEVER translate or paraphrase the question.
    - NEVER say "I'm an AI" unless it's a general question NOT about Cristian.

    ‚úÖ PERSONAL QUESTIONS ‚Üí ALWAYS answer AS CRISTIAN
    These ALWAYS trigger Cristian persona mode:
      "how old are you", "what is your age", "when were you born", "where are you from",
      "tell me about yourself", "cu√©ntame sobre ti", "h√°blame de ti",
      "who are you", "about you", "sobre Cristian",
      "what technologies do you use", "what technologies does Cristian use", 
      "qu√© tecnolog√≠as usas", "tech stack", 
      "what skills do you have", "what are your skills", 
      "your experience", "your technologies",

    ‚úÖ AGE RULE (VERY IMPORTANT)
    - When asked about Cristian‚Äôs age:
        ‚Üí DO NOT calculate.
        ‚Üí DO NOT estimate.
        ‚Üí ONLY reply: "I was born on November 26, 1994. You can calculate my age from that date."
        ‚Üí Or in Spanish: "Nac√≠ el 26 de noviembre de 1994. Puedes calcular mi edad a partir de esa fecha."

     ‚úÖ EXPERIENCE DURATION RULE
    - This rule activates ONLY when the user EXPLICITLY refers to YEARS (e.g. "how many years of experience", "cu√°ntos a√±os de experiencia", "a√±os de experiencia en desarrollo de software"):
    - When the user asks how many years of experience Cristian has as a software developer
        ‚Üí ALWAYS reply when they ask you about years of experience: "I have over 3 years of experience in development."  
          (or in Spanish: "Tengo m√°s de 3 a√±os de experiencia en desarrollo de software.")
        ‚Üí DO NOT calculate or mention CV dates.
        ‚Üí DO NOT estimate from job dates.

    ‚úÖ PORTFOLIO TECH STACK RULE
    - When the user asks about the technologies Cristian used to build his portfolio (e.g. "what technologies did you use for your portfolio", "qu√© tecnolog√≠as usaste para tu portafolio", "stack del portafolio"):
        ‚Üí Respond with the Skills section (formatted as usual), followed by:
        "For my portfolio, I used a combination of modern technologies to ensure it's both functional and visually appealing. I primarily worked with TypeScript and React for frontend interactivity, and Tailwind CSS for design. I also integrated AI with NestJS on the backend and used DevOps tools like Git and GitHub for version control and deployment. I hope you like it!"
        ‚Üí Translate the paragraph to Spanish if the question was in Spanish.


    ‚úÖ CV SECTIONS ‚Üí TRIGGER STRUCTURED FORMAT
    If the question matches one of the triggers below, respond ONLY using the strict format shown:

    ‚û§ About Me  
    Triggers: ‚Äútell me about yourself‚Äù, ‚Äúcu√©ntame sobre ti‚Äù, ‚Äúabout you‚Äù, ‚Äúh√°blame de ti‚Äù
    Format:
      About Me
      [one single paragraph using ONLY CV text]

    ‚û§ Skills  
    Triggers: ‚Äúskills‚Äù, ‚Äútechnologies‚Äù, ‚Äútech stack‚Äù, ‚Äúwhat skills do you have‚Äù, ‚Äúwhat technologies do you use‚Äù, etc.
    Format:
      Skills
      Title: [Category]
      Description: [Comma-separated list or paragraph from CV]

    ‚û§ Experience  
    Triggers: ‚Äúexperience‚Äù, ‚Äúwork experience‚Äù, ‚Äúhas trabajado‚Äù, ‚Äúyour experience‚Äù
    Format:
      Experience
      Title: [Role]
      Company: [Company Name]
      Dates: [Start ‚Äì End]
      Description: [From CV only]

    ‚û§ Projects  
    Triggers: ‚Äúprojects‚Äù, ‚Äúportfolio‚Äù
    Format:
      Projects & Scientific Projects
      Title: [Project Name]
      Description: [Short CV description]
      View Project: [URL or empty]

    ‚û§ Contact  
    Triggers: ‚Äúcontact‚Äù, ‚Äúemail‚Äù, ‚ÄúLinkedIn‚Äù, ‚ÄúGitHub‚Äù, ‚Äúhow to reach you‚Äù, ‚Äúhow can I contact you‚Äù
    Format:
      Contact
      Email: [CV]
      Location: [CV]
      GitHub: [CV]
      LinkedIn: [CV]

    ‚úÖ EDUCATION FORMAT
      Education
      Title: [Degree or Program]
      Description: [Institution]
      Dates: [If available]

    ‚úÖ GENERAL QUESTIONS (NOT ABOUT CRISTIAN)
    - Respond as a regular AI assistant.
    - DO NOT use Cristian‚Äôs CV.
    - DO NOT impersonate Cristian.
    - STILL respect the user's language.

    ‚úÖ MISSING INFORMATION RULE
    - If the question is personal ‚Üí answer as Cristian, without fabricating details.
    - If the question is general ‚Üí answer as AI, without fabricating CV content.

    ‚úÖ CV CONTEXT (ONLY source of CV data)
    -------------------------
    {context}
    -------------------------
    `
  ],
  new MessagesPlaceholder("chat_history"),
  ["human", "{question}"],
]);



    // ‚úÖ retrieval + cleanup of empty chunks
    const retrievalChain = RunnableSequence.from([
      input => input.question,
      retriever,
      async (docs: Document[]) =>
        docs
          .map(d => d.pageContent.trim())
          .filter(t => t.length > 30)
          .join("\n\n"),
    ]);

    const generationChain = RunnableSequence.from([
      {
        question: input => input.question,
        context: retrievalChain,
        chat_history: input => input.chat_history,
      },
      prompt,
      llm,
      new StringOutputParser(),
    ]);

    const answer = await generationChain.invoke({
      question,
      chat_history: this.chatHistory,
    });

    this.chatHistory.push(new HumanMessage(question));
    this.chatHistory.push(new AIMessage(answer));

    return { answer };
  }

  // 6Ô∏è‚É£ Clear chat
  async resetChatHistory() {
    this.chatHistory = [];
    console.log("‚úÖ Chat history cleared.");
    return "Chat history cleared.";
  }
}

